

# 项目总结

近期主要完成了以下几项任务。

## pagerank算法修改

我们的项目采用异构算法，项目能正常运行的前提是，不同端（CPU端和NPU端）对同一份数据的计算结果一致。这一点在之前的BFS算法很容易做到，因为BFS的计算结果是精确值，即使采用不同的实现逻辑，只要参数确定，计算结果就是确定的。但是pagerank算法是计算每个节点重要程度的近似值，采用不同的实现逻辑计算得到的值是不同的。之前用矩阵向量乘实现的NPU端pagerank算法和CPU端的逻辑不完全相同，所以又重写了这部分代码。

目前的pagerank算法逻辑是按照CPU端算法逻辑实现的，采用了增量计算的方式，其实现逻辑如下：

1. 初始化pagerank、delta和residual数组。pagerank数组表示每个节点的当前分数。residual数组表示每个节点汇聚到的“邻居节点传递给该节点的增量分数”。delta数组表示该节点传递给邻居节点的分数增量。
2. 对于图中的每个节点，如果该节点的residual值大于TOLERANCE，则更新pagerank和delta值。pagerank值等于原有的pagerank值加上residual增量。delta值等于该节点的residual值乘以ALPHA除以该节点的出度。
3. 更新每个节点的residual值，它等于入度邻居的delta值之和
4. 重复步骤2和3，直到分数收敛（所有节点的residual增量都小于收敛阈值）或达到最大迭代次数。

在每次迭代中，pagerank数组包含每个节点的当前分数。residual数组包含每个节点在当前迭代中未被传递的分数。delta数组包含从其他节点传递给该节点的分数增量。

## 数据传输代码修改

之前的数据传输代码在传输大数据量时会阻塞，因为我们采用了管道传输的方式，而linux系统默认的管道容量是64kb，一旦超过64kb，管道就会阻塞。现在采用的方式如下：

在Python代码中，`write_pagerank`函数使用了管道来将`pagerank`字典数据传输到C++端。首先，它创建了一个管道，然后将管道写入端设置为非阻塞模式。接下来，它将字典数据序列化为字符串，并使用分批的方式将数据写入管道。在写入具体数据之前它会先写入总数据量和分块大小，然后以分块为单位传输，每传输完一个分块它会自动阻塞，直到分块被读取。当所有的数据都传送完毕后，它会关闭管道写入端的文件对象，并关闭管道的读取端。

在C++代码中，`main`函数首先创建了一个管道，并创建了一个子进程。子进程中执行Python程序，通过设置环境变量`PIPE_FD`传递管道写入端的文件描述符给Python程序。父进程在读取管道前，会先关闭管道的写入端，然后通过循环调用`read_data_from_pipe`函数来读取管道数据。首先读取总数据量和分块数据量，然后每次读完一个分块的数据，直到读取完所有数据为止。每次读完一个分块后，它都会清空管道，以确保Python端可以继续写入。

## 使用c++重写图重排序代码

为了将之前完成的单独的工作集成到现有项目，用c++语言重写了图重排序算法，

## 代码集成

主要是修改DepGraph，完善它与npu端交互的逻辑

